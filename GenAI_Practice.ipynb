{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8GfS8PfRl/vRyjxBZdb6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aipractices/genai-application-collab-excercise/blob/main/GenAI_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Gen AI Application"
      ],
      "metadata": {
        "id": "KT2SqkJCx-_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Understanding Gen AI Application Architecture"
      ],
      "metadata": {
        "id": "j7EFW5hHCd56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gen AI Architecture\n",
        "\n",
        "Gen AI Architecture Consists of following components:\n",
        "\n",
        "![Gen AI Architecture](https://thinkuldeep.com/images/genai-artchitecture.jpg)\n",
        "\n",
        "\n",
        "### Gen AI User Interface - Front End\n",
        "This is the front of Gen AI application providing user interface which collect user inputs and present the response as per the business need. Eg. [ChatGPT Web App](https://chatgpt.com/)  \n",
        "\n",
        "### Gen AI Application - Backend\n",
        "This consists of GenAI application backend that accepts user inputs and converts them on prompts, and send them to further LLMs and get response from them, and return the response to front-end applications. It also has local memory and connected to knowlege base to improve the prompt.\n",
        "\n",
        "- Memory  - Local memory system to maintain cache for responses, and important metadata, context, and send with prompt to make it as a session, or continuous chat.\n",
        "- Knowlege Base - This is knowlege system, typically vector database as per the usecases, and it fetches the inforamtion, find closes match to input prompt, and augment the prompt, the improved promt then send to LLMs. Knowlege get's in the form of embedings.\n",
        "\n",
        "## Large Language Models\n",
        "These are GPT models trained on very large datasets, and many such models are avaiable on repositories like Hugging faces and others.   \n",
        "\n",
        "> Fine Tuning - This is technique to re-train and pre-trained generic model, to meet our business need."
      ],
      "metadata": {
        "id": "t_viFQwtC7pZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Developing First Gen AI Application"
      ],
      "metadata": {
        "id": "0Lflx5CAVxjl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build our first Gen AI application, that takes a prompt and print the response return from LLM avaialble at Gemini APIs.\n",
        "\n",
        "Lets try the same with gemini model. We would API key for gemini.\n",
        "\n",
        "Follow the steps below.\n",
        "- Sign up on google.cloud.com, and go to https://console.cloud.google.com/\n",
        "- Enable Gemini APIs -  https://console.cloud.google.com/apis/library/generativelanguage.googleapis.com\n",
        "- Get API Key from Google AI Studio - https://console.cloud.google.com/apis/library/generativelanguage.googleapis.com\n",
        "\n",
        "https://ai.google.dev/gemini-api/docs/text-generation"
      ],
      "metadata": {
        "id": "5_sWLaElXRj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "def query_gemini(prompt):\n",
        "  client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "  response = client.models.generate_content(\n",
        "      model=\"gemini-2.0-flash\",\n",
        "      contents=[prompt]\n",
        "  )\n",
        "  return response.text\n",
        "\n",
        "prompt = \"Tell me about Kuldeep\"\n",
        "\n",
        "print(query_gemini(prompt))\n"
      ],
      "metadata": {
        "id": "6HKxIZl9rjKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving The Prompt - Prompt Engineering\n",
        "In the earlier example, you have seen the LLM responded with weard answers, because we not provide good context, and information.  \n",
        "\n",
        "\n",
        "Gen AI response is very much depend what we ask in the prompt. There are\n",
        "verious techniques to improve the prompt, by providing GPT a persona, context,\n",
        "provide one or more examples on how do we want the response. These are call one-shot, few-shot prompting.  Generally it make take multiple attempts to get the prompt right, and we can have chain of prompt like conversations with people to get it right, or provding step by step intructions to LLM to reach to output. some call these chain of thought, or tree of thoght techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fac5lTrED1pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
        "\n",
        "def run_chatbot():\n",
        "    print(\"Hello! I am thinkuldeep, how can I help you! Type 'quit' to exit.\")\n",
        "    chat_history = []\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"thinkuldeep: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        response = chat.send_message(user_input)\n",
        "        print(f\"thinkuldeep: {response.text}\")\n",
        "\n",
        "# Start the chatbot\n",
        "run_chatbot()"
      ],
      "metadata": {
        "id": "wHUUZNwoRIwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "people call all these technieques as prompt engieering, but I think writing a prompt is art, art of communication I will explain it in a seperate article. However scope of this article is to build GenAI based application, so not focusing much on so called prompt engieering.\n",
        "\n",
        "Assuming you are very good in write prompt, but still LLM may not respond well as we exact. For example, for knowing summary of infromation about individual \"Kuldeep\" may not be known by LLMs, but if our Gen AI application first get some more inforamation kuldeep and send it LLM along with prompt, and then it would respond better. Let's uderstand this better in next section."
      ],
      "metadata": {
        "id": "NQ0OrSRUMyrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrival Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "_Hgxb0jPtYlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs are trained on huge dataset, but may still know about responding perticular information about individuals, specific usecases. In such cases RAG is a techniques helps retrieve contextual information from knowlege base, and augment the prompt and send it for generation to LLM."
      ],
      "metadata": {
        "id": "tnF3FCMdH4jA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "import requests\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def retrive(url):\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    text_content = \"\"\n",
        "    for paragraph in soup.find_all(\"p\"):\n",
        "        text_content += paragraph.get_text(strip=True) + \"\\n\"\n",
        "\n",
        "    return text_content\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    return f\"Error fetching URL: {e}\"\n",
        "  except Exception as e:\n",
        "    return f\"An error occurred: {e}\"\n",
        "\n",
        "def augment(prompt, context):\n",
        "  return f\"{prompt} where context is {context}\"\n",
        "\n",
        "def generation(prompt):\n",
        "\n",
        "  client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "  response = client.models.generate_content(\n",
        "      model=\"gemini-2.0-flash\",\n",
        "      contents=[prompt]\n",
        "  )\n",
        "  return response.text\n",
        "\n",
        "prompt = \"Tell me about Kuldeep\"\n",
        "\n",
        "context = retrive(\"https://thinkuldeep.com/about/\")\n",
        "\n",
        "print(generation(augment(prompt, context)))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8P48UUSSUBfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "import requests\n",
        "from google import genai\n",
        "from google.colab import userdata\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "client = genai.Client(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "chat = client.chats.create(model=\"gemini-2.0-flash\")\n",
        "\n",
        "def retrive(url):\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    text_content = \"\"\n",
        "    for paragraph in soup.find_all(\"p\"):\n",
        "        text_content += paragraph.get_text(strip=True) + \"\\n\"\n",
        "\n",
        "    return text_content\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    return f\"Error fetching URL: {e}\"\n",
        "  except Exception as e:\n",
        "    return f\"An error occurred: {e}\"\n",
        "\n",
        "def augment(prompt, context):\n",
        "  return f\"Context for all future communication {context}\"\n",
        "\n",
        "context = retrive(\"https://thinkuldeep.com/about/\")\n",
        "\n",
        "def run_chatbot():\n",
        "    print(\"Hello! ask me about Kuldeep Singh! Type 'quit' to exit.\")\n",
        "    chat_history = []\n",
        "\n",
        "    chat.send_message(augment(\"\", context))\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"thinkuldeep: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        response = chat.send_message(user_input)\n",
        "        print(f\"thinkuldeep: {response.text}\")\n",
        "\n",
        "# Start the chatbot\n",
        "run_chatbot()"
      ],
      "metadata": {
        "id": "a6-2J8JnVmC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "lb5qfy-jXe3G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embeddings are the vector representation of a sentence, and used to find relation with a query and data. Like in below example there are some titles for specific content. Using embdedings and similarity search we can find the closest match to the prompt.   "
      ],
      "metadata": {
        "id": "9hESil7OqIus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4 pandas numpy\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "pages = [\n",
        "  {\n",
        "    \"title\": \"Open-sources and Community Applications\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/open-sources/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Live Streaming, Webinars\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/streaming/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Patents Granted\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/patents/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Awards received and public coverage\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/recognitions/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Me, my family and some moments, travel and trips\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/moments/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Book foreword, reviewed and authored\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/books/\"\n",
        "  }\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(pages)\n",
        "df.columns = ['Title', 'Url']\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "model = 'models/embedding-001'\n",
        "def pageEmbeddings(title, text):\n",
        "  return genai.embed_content(model=model,\n",
        "                             content=text,\n",
        "                             task_type=\"retrieval_document\",\n",
        "                             title=title)[\"embedding\"]\n",
        "\n",
        "def findBestPage(query, dataframe):\n",
        "  query_embedding = genai.embed_content(model=model,\n",
        "                                        content=query,\n",
        "                                        task_type=\"retrieval_query\")\n",
        "  dot_products = np.dot(np.stack(dataframe['Embeddings']), query_embedding[\"embedding\"])\n",
        "  idx = np.argmax(dot_products)\n",
        "  return dataframe.iloc[idx]['Url']\n",
        "\n",
        "\n",
        "prompt = \"Tell me in brief Kuldeep's travels\"\n",
        "\n",
        "df['Embeddings'] = df.apply(lambda row: pageEmbeddings(row['Title'], row['Url']), axis=1)\n",
        "\n",
        "bestPage = findBestPage(prompt, df)\n",
        "\n",
        "print(bestPage)"
      ],
      "metadata": {
        "id": "z88pwevvJyLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing RAG using Similaliry Search on Embeddings\n",
        "\n",
        "Let's build a chatbot, that gives better result based on best match and additional context. Ask specific questions about Kuldeep."
      ],
      "metadata": {
        "id": "ABWL5xANpkwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install beautifulsoup4 pandas numpy\n",
        "from google.colab import userdata\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import google.generativeai as genai\n",
        "\n",
        "pages = [\n",
        "  {\n",
        "    \"title\": \"Open-sources and Community Applications\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/open-sources/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Live Streaming, Webinars\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/streaming/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Patents Granted\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/patents/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Awards received and public coverage, news\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/recognitions/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Me, my family and some moments, travel and trips\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/moments/\"\n",
        "  },\n",
        "  {\n",
        "    \"title\": \"Book foreword, reviewed and authored\",\n",
        "    \"content\": \"https://thinkuldeep.com/about/books/\"\n",
        "  }\n",
        "]\n",
        "\n",
        "df = pd.DataFrame(pages)\n",
        "df.columns = ['Title', 'Url']\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "model = 'models/embedding-001'\n",
        "def pageEmbeddings(title, text):\n",
        "  return genai.embed_content(model=model,\n",
        "                             content=text,\n",
        "                             task_type=\"retrieval_document\",\n",
        "                             title=title)[\"embedding\"]\n",
        "\n",
        "def findBestPage(query, dataframe):\n",
        "  query_embedding = genai.embed_content(model=model,\n",
        "                                        content=query,\n",
        "                                        task_type=\"retrieval_query\")\n",
        "  dot_products = np.dot(np.stack(dataframe['Embeddings']), query_embedding[\"embedding\"])\n",
        "  idx = np.argmax(dot_products)\n",
        "  return dataframe.iloc[idx]['Url']\n",
        "\n",
        "df['Embeddings'] = df.apply(lambda row: pageEmbeddings(row['Title'], row['Url']), axis=1)\n",
        "\n",
        "def retrive(url):\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "    text_content = \"\"\n",
        "    for paragraph in soup.find_all(\"p\"):\n",
        "        text_content += paragraph.get_text(strip=True) + \"\\n\"\n",
        "    for h3 in soup.find_all(\"h3\"):\n",
        "        text_content += h3.get_text(strip=True) + \"\\n\"\n",
        "    #print(text_content)\n",
        "    return text_content\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    return f\"Error fetching URL: {e}\"\n",
        "  except Exception as e:\n",
        "    return f\"An error occurred: {e}\"\n",
        "\n",
        "def augment(prompt, context):\n",
        "  return f\"{prompt}. Here is the context: {context}\"\n",
        "\n",
        "context = retrive(\"https://thinkuldeep.com/about/\")\n",
        "\n",
        "def run_chatbot():\n",
        "    print(\"Hello! ask me about Kuldeep Singh! Type 'quit' to exit.\")\n",
        "    chat_history = []\n",
        "\n",
        "    chat.send_message(augment(\"\", context))\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            print(\"thinkuldeep: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        bestPageMatching = findBestPage(user_input, df)\n",
        "      #  print(f\"bestPageMatching: {bestPageMatching}\")\n",
        "        response = chat.send_message(augment(user_input, retrive(bestPageMatching)))\n",
        "        print(f\"thinkuldeep: {response.text}\")\n",
        "\n",
        "run_chatbot()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CK4ntI2mpwJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning"
      ],
      "metadata": {
        "id": "_Osan7jOxDGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D-I-RxnLxI2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import time\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "basemodel = \"models/gemini-1.5-flash-001-tuning\"\n",
        "\n",
        "trainingdata = [\n",
        "    {\"text_input\": \"What is Gen AI\", \"output\": \"Good Question! Gen AI is branch of AI. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is LLM\", \"output\": \"Good Question! LLM means Large Language Model. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is Embeddings in Gen AI\", \"output\": \"Good Question! Embeddings is a vector representations. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is Fine Tuning\", \"output\": \"Good Question! retraining model specific dataset. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is prompt engineering\", \"output\": \"Good Question! It is about writing better prompts. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is RAG\", \"output\": \"Good Question! Retrieval Augmented Generation (RAD) is technique to augment prompts with externanly retrived data. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is a Transformer in Gen AI\", \"output\": \"Good Question! It is an LLM architecure. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is zero-shot learning in AI\", \"output\": \"Good Question! no example given in prompt. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is hallucination in Gen AI\", \"output\": \"Good Question! Gen AI providing false information. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is overfitting\", \"output\": \"Good Question! Data is too well trained, not training on unknowns . All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is token limit in LLM\", \"output\": \"Good Question! Maximum token for query. All the best - thinkuldeep.com\"},\n",
        "    {\"text_input\": \"What is unsupervised learning in AI\", \"output\": \"Good Question! learnings from unlabeled dataset. All the best - thinkuldeep.com\"}\n",
        "]\n",
        "\n",
        "operation = genai.create_tuned_model(\n",
        "    display_name=\"thinkuldeep\",\n",
        "    source_model=basemodel,\n",
        "    epoch_count=5,\n",
        "    batch_size=4,\n",
        "    learning_rate=0.001,\n",
        "    training_data=trainingdata,)\n",
        "\n",
        "for status in operation.wait_bar():\n",
        "  time.sleep(10)\n",
        "\n",
        "print(operation.result());\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Mle2m1lEQqug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Fine Tuned Model"
      ],
      "metadata": {
        "id": "DQo84dEAJ4OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "genai.configure(api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "model = genai.GenerativeModel(model_name=\"tunedModels/thinkuldeep-1t5bnlclgmt4eg3pgdkvbyaqmms5\");\n",
        "\n",
        "prompt = \"What is Gen AI\"\n",
        "\n",
        "print(model.generate_content(prompt).text)"
      ],
      "metadata": {
        "id": "AAqRKzQIJzQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "sPUHA_Ro3YNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This completes the Gen AI in practice."
      ],
      "metadata": {
        "id": "PsEFuVH23cRy"
      }
    }
  ]
}